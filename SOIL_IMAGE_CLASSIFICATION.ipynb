{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c946f3dc-3223-42ee-8da2-c876da0a8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import pathlib\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc8ded79-89e5-45df-aba8-303727b6beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make a sequence of image transformations before processing\n",
    "# This includes converting image to tensors, normalising pixel values and resizing images\n",
    "transform = transforms.Compose([\n",
    "    # Convert image to a PyTorch tensor:(basically scaling it down to 0 to 1)\n",
    "    transforms.ToTensor(),\n",
    "    # This function standardizes pixel values of imgs using given mean and std deviation as parameters(normalized_pixel = pixel-mean/std)(first parameter is for mean, second for std deviaiton)\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)) ,\n",
    "    # This function resizes the image to the specified size\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b55f201-100c-490a-9f3d-fbde53fc9ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I defined directories for train and test data\n",
    "train_dir = '/home/goodarth/Desktop/IEEE SPECTRO/Soil types'\n",
    "test_dir = '/home/goodarth/Desktop/IEEE SPECTRO/Soil_types_test'\n",
    "\n",
    "# Loaded training and test datasets using ImageFolder, pointing to a specific local folder.\n",
    "# THe images are also transformed as mentioned above as they are loaded\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd5cd537-0843-44d5-bc2b-22c35995698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for training and test data\n",
    "# Data Loaders enable optimised loading of datasets for training and testing(by batching,shufflinf and parallel loading)\n",
    "# The batch size is set to 5, which means at once 5 images are loaded\n",
    "# Shuffle parameter mentions if the daataset is shuffled or not before each epoch\n",
    "# num_workers parameter controls the number of CPU processes that load data in parallel while the model trains\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=5, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0644dce5-4a3d-46a8-8f59-c262443e60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i defined the categories of images in the dataset for classification purposes\n",
    "class_images =['Black Soil','Cinder Soil','Laterite Soil','Peat Soil','Yellow Soil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e4537e4-d9d7-4fbd-8d1c-a940c1cb9f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here defined a CNN architecture by creating a class using nn.module from pytorch\n",
    "# nn.module serves as a base class for all neuralnets, providing an extensive franework for defining and organizing the layers and functions of a neural network\n",
    "class NeuralNetCNN(nn.Module):\n",
    "    # this is the constructor method/function whichgets called on making every new instance of the class.\n",
    "    def __init__(self, num_class_images = 6):\n",
    "        # Initializes the paraent class\n",
    "        # This is a necessary step alloing pytorch to initialise settings and proerties in the nn.module\n",
    "        super(NeuralNetCNN,self).__init__()\n",
    "\n",
    "        #Output size after convolution filter\n",
    "        #((w-f+2p)/s) + 1\n",
    "        # w = width image, f = filter width , p = padding width, s = stride\n",
    "        # conv1/2/3 is notation for the nth CNN\n",
    "        # in_channels specifies the number of colour channels used( like RGB)\n",
    "        # out_channels specifies the number of filters used/ the number of resulting output channels\n",
    "        # kernel_size specifies filter dimension size\n",
    "        # stride specifies the stride of the filter(the number of steps it takes while moving the filter across the image)\n",
    "        # padding specifies the padding size of the filter around the image, which is necessary to prevent the image from shrinking, and lead to quality drop in further steps\n",
    "        # bn1/2/3 represent batch normalisation function which normalises the data in every layer post output\n",
    "        # relu1/2/3 represent the activation function used in every layer post output(ReLU)\n",
    "\n",
    "\n",
    "        # FIRST layer of the CNN\n",
    "        # Input shape = (256,3,150,150)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n",
    "        # Now shape = (256,12,150,150)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=12)\n",
    "        # Now shape = (256,12,150,150)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # Now shape = (256,12,150,150)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        # Now shape = (256,12,75,75)\n",
    "\n",
    "        # SECOND layer of CNN\n",
    "        self.conv2 = nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
    "        # Now shape = (256,20,75,75)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=20)\n",
    "        # Now shape = (256,20,75,75)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Now shape = (256,20,75,75)\n",
    "\n",
    "\n",
    "        # THIRD layer of CNN\n",
    "        self.conv3 = nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        # Now shape = (256,32,75,75)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        # Now shape = (256,32,75,75)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # Now shape = (256,32,75,75)\n",
    "\n",
    "        # this is the fully connected layer, which makes a single column matrix by stacking all data of the final matrix post the 3rd layer\n",
    "        self.fc = nn.Linear(in_features=32*75*75,out_features=num_class_images)\n",
    "\n",
    "    # This is the forward pass of the network, it is the function that is called when the network is run\n",
    "    def forward(self,input):\n",
    "        # applying all layers in order\n",
    "        output =self.conv1(input)\n",
    "        output =self.bn1(output)\n",
    "        output =self.relu1(output)\n",
    "\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        output =self.pool(output)\n",
    "        output =self.conv2(output)\n",
    "        output =self.bn2(output)\n",
    "        output =self.relu2(output)\n",
    "        output =self.conv3(output)\n",
    "        output =self.bn3(output)\n",
    "        output =self.relu3(output)\n",
    "\n",
    "        output = output.view(-1,32*75*75)\n",
    "        output =self.fc(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f59f15c-83cb-4e6f-b301-27c175fbb17f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# loss calculation between predictions and true labels\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# gradient computation (backprop)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)"
     ]
    }
   ],
   "source": [
    "# This line sets the computation device to GPU if available, otherwise CPU.\n",
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# creates an instant of the defined CNN achitecture and transfers it to chosen device\n",
    "model = NeuralNetCNN(num_class_images = 5).to(device)\n",
    "\n",
    "# made use of adam optimizer\n",
    "# model.parameters() passes the model’s learnable parameters (weights and biases) to the optimizer.\n",
    "# learning rate is the step size for adjusting weights with each optimization step. In Adam, it controls the general rate of updates to the model's parameters.\n",
    "# weight decay is a form of regularisation which helps prevent overfitting, Setting weight_decay=0.0001 means that a small penalty is applied to the model weights, encouraging smaller weights and potentially improving the model’s generalization to new data.\n",
    "optimizer = Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "\n",
    "# loss function used is crossentropyloss\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "\n",
    "#train_count = len(glob.glob(train_dir+'/**/*.jpg',recursive=True))\n",
    "#test_count = len(glob.glob(test_dir+'/**/*.jpg'))\n",
    "#print(test_count,train_count)\n",
    "#print(test_dir,train_dir)\n",
    "\n",
    "\n",
    "best_accuracy = 0.0\n",
    "#  for loop which runs for a specified number of epochs.\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_accuracy = 0.0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # for loop over batches in training data\n",
    "    for batch_index, (images, labels) in enumerate(train_loader):\n",
    "        # helps clear previous grads before backprop\n",
    "        optimizer.zero_grad()\n",
    "        # passes images through the model\n",
    "        outputs = model(images.to(device))\n",
    "        # loss calculation between predictions and true labels\n",
    "        loss = loss_function(outputs,labels)\n",
    "        # gradient computation (backprop)\n",
    "        loss.backward()\n",
    "        # updation of parameters\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        # calculating accuracy\n",
    "        train_loss = train_loss + loss.cpu().data*images.size(0)\n",
    "        _,prediction = torch.max(outputs.data,1)\n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "    # calculating accuracy over entire dataset\n",
    "    train_accuracy = train_accuracy/len(train_loader.dataset)\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    test_accuracy = 0.0\n",
    "    for batch_index, (images, labels) in enumerate(test_loader):\n",
    "        outputs = model(images.to(device))\n",
    "        # torch.max(outputs,data,1) gets the predicted class from the data.\n",
    "        _,prediction = torch.max(outputs.data,1)\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "    test_accuracy = test_accuracy/len(test_loader.dataset)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1} Train Loss: {train_loss:.4f} Train Accuracy: {train_accuracy:.4f} Test Accuracy: {test_accuracy:.4f}')\n",
    "    \n",
    "    # print('Epoch: '+str(epoch)+'Train Loss: '+str(float(train_loss))+'Train Accuracy: '+str(float(train_accuracy))+'Test Accuracy: '+str(float(test_accuracy)))\n",
    "\n",
    "    #saving the best model if accuracy improves over epochs.\n",
    "    if test_accuracy>best_accuracy:\n",
    "        torch.save(model.state_dict(),'best_epoch.model')\n",
    "        best_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3277691-b6f7-48ae-937f-77d5b73b34e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('yo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cdd5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
